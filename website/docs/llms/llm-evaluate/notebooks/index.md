# LLM Evaluation Examples {#llm-evaluation-examples}

The notebooks listed below contain step-by-step tutorials on how to use
MLflow to evaluate LLMs. The first notebook is centered around
evaluating an LLM for question-answering with a prompt engineering
approach. The second notebook is centered around evaluating a RAG
system. Both notebooks will demonstrate how to use MLflow's builtin
metrics such as token_count and toxicity as well as LLM-judged
intelligent metrics such as answer_relevance.

<div class="toctree" markdown="1" maxdepth="1" hidden="">

question-answering-evaluation.ipynb rag-evaluation.ipynb

</div>

## QA Evaluation Notebook {#qa-evaluation-notebook}

If you would like a copy of this notebook to execute in your
environment, download the notebook here:

<a href="https://raw.githubusercontent.com/mlflow/mlflow/master/docs/source/llms/llm-evaluate/notebooks/question-answering-evaluation.ipynb" class="notebook-download-btn">Download the notebook</a><br/>

To follow along and see the sections of the notebook guide, click below:

<a href="question-answering-evaluation.html" class="download-btn">View the Notebook</a><br/>

## RAG Evaluation Notebook {#rag-evaluation-notebook}

If you would like a copy of this notebook to execute in your
environment, download the notebook here:

<a href="https://raw.githubusercontent.com/mlflow/mlflow/master/docs/source/llms/llm-evaluate/notebooks/rag-evaluation.ipynb" class="notebook-download-btn">Download the notebook</a><br/>

To follow along and see the sections of the notebook guide, click below:

<a href="rag-evaluation.html" class="download-btn">View the Notebook</a><br/>
