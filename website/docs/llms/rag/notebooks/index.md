# Question Generation for Retrieval Evaluation {#question-generation-for-retrieval-evaluation}

This notebook is a step-by-step tutorial on how to generate a question
dataset with LLMs for retrieval evaluation within RAG. It will guide you
through getting a document dataset, generating diverse and relevant
questions through prompt engineering on LLMs, and analyzing the question
dataset. The question dataset can then be used for the subsequent task
of evaluating the retriever model, which is a part of RAG that collects
and ranks relevant document chunks based on the user's question.

<div class="toctree" markdown="1" maxdepth="1" hidden="">

question-generation-retrieval-evaluation.ipynb

</div>

## Question Generation for RAG Notebook {#question-generation-for-rag-notebook}

If you would like a copy of this notebook to execute in your
environment, download the notebook here:

<a href="https://raw.githubusercontent.com/mlflow/mlflow/master/docs/source/llms/rag/notebooks/question-generation-retrieval-evaluation.ipynb" class="notebook-download-btn">Download the notebook</a><br/>

To follow along and see the sections of the notebook guide, click below:

<a href="question-generation-retrieval-evaluation.html" class="download-btn">View the Notebook</a><br/>
