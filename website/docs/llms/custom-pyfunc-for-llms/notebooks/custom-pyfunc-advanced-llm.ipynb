{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving LLMs with MLflow: Leveraging Custom PyFunc\n",
    "\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "In this tutorial, we'll explore how to save custom cutting-edge Large Language Models (LLMs) using MLflow. Specifically, we'll delve into the intricacies of a situation where the default MLflow 'transformers' flavor does not provide direct support for our model type and its dependencies. This necessitates the creation of a custom `pyfunc` to ensure seamless model deployment. \n",
    "\n",
    "Through this tutorial, we aim to provide you with:\n",
    "\n",
    "- An understanding of why certain models might need custom `pyfunc` definitions.\n",
    "\n",
    "- A walk-through of creating a custom `pyfunc` to handle model dependencies and interface data.\n",
    "\n",
    "- Insight into how a custom `pyfunc` can offer a simplified interface to end-users in a deployed environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Challenge with Default Implementations\n",
    "\n",
    "MLflow's `transformers` flavor provides a standardized way to handle models from the HuggingFace Transformers library. However, not all models or configurations might fit neatly into this standardized format. \n",
    "\n",
    "In our scenario, the model cannot use the default `pipeline` type due to certain incompatibilities. This poses a challenge: how do we ensure that our model can be saved, loaded, and served using MLflow, given these constraints?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Power of Custom PyFunc\n",
    "\n",
    "The solution lies in MLflow's ability to define custom `pyfunc`. By creating a custom `pyfunc`, we can:\n",
    "\n",
    "- Define how the model loads its dependencies.\n",
    "\n",
    "- Customize the inference process.\n",
    "\n",
    "- Manipulate interface data to create specific inputs for the model.\n",
    "\n",
    "Let's dive into the code to see this in action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Considerations Before Proceeding\n",
    "\n",
    "#### Hardware Recommendations\n",
    "This guide demonstrates the usage of a particularly large and intricate Large Language Model (LLM). Given its complexity:\n",
    "\n",
    "- **GPU Requirement**: It's **strongly advised** to run this example on a system with a CUDA-capable GPU that possesses at least 64GB of VRAM.\n",
    "- **CPU Caution**: While technically feasible, executing the model on a CPU can result in extremely prolonged inference times, potentially taking tens of minutes for a single prediction, even on top-tier CPUs. The final cell of this notebook is deliberately not executed due to the limitations with performance when running this model on a CPU-only system. However, with an appropriately powerful GPU, the total runtime of this notebook is ~8 minutes end to end.\n",
    "\n",
    "#### Execution Recommendations\n",
    "If you're considering running the code in this notebook:\n",
    "\n",
    "- **Performance**: For a smoother experience and to truly harness the model's capabilities, use hardware aligned with the model's design.\n",
    "\n",
    "- **Dependencies**: Ensure you've installed the recommended dependencies for optimal model performance. These are crucial for efficient model loading, initialization, attention computations, and inference processing:\n",
    "\n",
    "```bash\n",
    "pip install xformers==0.0.20 einops==0.6.1 flash-attn==v1.0.3.post0 triton-pre-mlir@git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python\n",
    "```\n",
    "\n",
    "### Learning Objectives\n",
    "Remember, while hands-on execution provides valuable insights, the primary aim of this guide is to illustrate the effective use of MLflow in the showcased workflow. If you're unable to run the notebook due to hardware constraints, you can still gain a comprehensive understanding by reviewing and analyzing the code and explanations provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "\n",
    "import transformers\n",
    "import mlflow\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the Model and Tokenizer\n",
    "\n",
    "First, we need to download our model and tokenizer. Here's how we do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd68b848a614c84b948e92519e7370d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 22 files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download the MPT-7B instruct model and tokenizer to a local directory cache\n",
    "snapshot_location = snapshot_download(repo_id=\"mosaicml/mpt-7b-instruct\", local_dir=\"mpt-7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Custom PyFunc\n",
    "\n",
    "Now, let's define our custom `pyfunc`. This will dictate how our model loads its dependencies and how it performs predictions. Notice how we've wrapped the intricacies of the model within this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPT(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        This method initializes the tokenizer and language model\n",
    "        using the specified model snapshot directory.\n",
    "        \"\"\"\n",
    "        # Initialize tokenizer and language model\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            context.artifacts[\"snapshot\"], padding_side=\"left\"\n",
    "        )\n",
    "\n",
    "        config = transformers.AutoConfig.from_pretrained(\n",
    "            context.artifacts[\"snapshot\"], trust_remote_code=True\n",
    "        )\n",
    "        # If you are running this in a system that has a sufficiently powerful GPU with available VRAM,\n",
    "        # uncomment the configuration setting below to leverage triton.\n",
    "        # Note that triton dramatically improves the inference speed performance\n",
    "\n",
    "        # config.attn_config[\"attn_impl\"] = \"triton\"\n",
    "\n",
    "        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            context.artifacts[\"snapshot\"],\n",
    "            config=config,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # NB: If you do not have a CUDA-capable device or have torch installed with CUDA support\n",
    "        # this setting will not function correctly. Setting device to 'cpu' is valid, but\n",
    "        # the performance will be very slow.\n",
    "        self.model.to(device=\"cpu\")\n",
    "        # If running on a GPU-compatible environment, uncomment the following line:\n",
    "        # self.model.to(device=\"cuda\")\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def _build_prompt(self, instruction):\n",
    "        \"\"\"\n",
    "        This method generates the prompt for the model.\n",
    "        \"\"\"\n",
    "        INSTRUCTION_KEY = \"### Instruction:\"\n",
    "        RESPONSE_KEY = \"### Response:\"\n",
    "        INTRO_BLURB = (\n",
    "            \"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\"\n",
    "        )\n",
    "\n",
    "        return f\"\"\"{INTRO_BLURB}\n",
    "        {INSTRUCTION_KEY}\n",
    "        {instruction}\n",
    "        {RESPONSE_KEY}\n",
    "        \"\"\"\n",
    "\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        \"\"\"\n",
    "        This method generates prediction for the given input.\n",
    "        \"\"\"\n",
    "        prompt = model_input[\"prompt\"][0]\n",
    "\n",
    "        # Retrieve or use default values for temperature and max_tokens\n",
    "        temperature = params.get(\"temperature\", 0.1) if params else 0.1\n",
    "        max_tokens = params.get(\"max_tokens\", 1000) if params else 1000\n",
    "\n",
    "        # Build the prompt\n",
    "        prompt = self._build_prompt(prompt)\n",
    "\n",
    "        # Encode the input and generate prediction\n",
    "        # NB: Sending the tokenized inputs to the GPU here explicitly will not work if your system does not have CUDA support.\n",
    "        # If attempting to run this with GPU support, change 'cpu' to 'cuda' for maximum performance\n",
    "        encoded_input = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "        output = self.model.generate(\n",
    "            encoded_input,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_tokens,\n",
    "        )\n",
    "\n",
    "        # Decode the prediction to text\n",
    "        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Removing the prompt from the generated text\n",
    "        prompt_length = len(self.tokenizer.encode(prompt, return_tensors=\"pt\")[0])\n",
    "        generated_response = self.tokenizer.decode(\n",
    "            output[0][prompt_length:], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        return {\"candidates\": [generated_response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Prompt\n",
    "\n",
    "One key aspect of our custom `pyfunc` is the construction of a model prompt. Instead of the end-user having to understand and construct this prompt, our custom `pyfunc` takes care of it. This ensures that regardless of the intricacies of the model's requirements, the end-user interface remains simple and consistent.\n",
    "\n",
    "Review the method `_build_prompt()` inside our class above to see how custom input processing logic can be added to a custom pyfunc to support required translations of user-input data into a format that is compatible with the wrapped model instance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, ColSpec, ParamSchema, ParamSpec\n",
    "\n",
    "# Define input and output schema\n",
    "input_schema = Schema(\n",
    "    [\n",
    "        ColSpec(DataType.string, \"prompt\"),\n",
    "    ]\n",
    ")\n",
    "output_schema = Schema([ColSpec(DataType.string, \"candidates\")])\n",
    "\n",
    "parameters = ParamSchema(\n",
    "    [\n",
    "        ParamSpec(\"temperature\", DataType.float, np.float32(0.1), None),\n",
    "        ParamSpec(\"max_tokens\", DataType.integer, np.int32(1000), None),\n",
    "    ]\n",
    ")\n",
    "\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=parameters)\n",
    "\n",
    "\n",
    "# Define input example\n",
    "input_example = pd.DataFrame({\"prompt\": [\"What is machine learning?\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the experiment that we're going to be logging our custom model to\n",
    "\n",
    "If the the experiment doesn't already exist, MLflow will create a new experiment with this name and will alert you that it has created a new experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/12 16:54:21 INFO mlflow.tracking.fluent: Experiment with name 'mpt-7b-instruct-evaluation' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/benjamin.wilson/repos/mlflow-fork/mlflow/docs/source/llms/custom-pyfunc-for-llms/notebooks/mlruns/528860847726625085', creation_time=1697144061460, experiment_id='528860847726625085', last_update_time=1697144061460, lifecycle_stage='active', name='mpt-7b-instruct-evaluation', tags={}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_name=\"mpt-7b-instruct-evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8893d82e2d474a7783e8fe4245874f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/12 16:54:21 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n",
      "/Users/benjamin.wilson/miniconda3/envs/mlflow-dev-env/lib/python3.8/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# Get the current base version of torch that is installed, without specific version modifiers\n",
    "torch_version = torch.__version__.split(\"+\")[0]\n",
    "\n",
    "# Start an MLflow run context and log the MPT-7B model wrapper along with the param-included signature to\n",
    "# allow for overriding parameters at inference time\n",
    "with mlflow.start_run():\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        \"mpt-7b-instruct\",\n",
    "        python_model=MPT(),\n",
    "        # NOTE: the artifacts dictionary mapping is critical! This dict is used by the load_context() method in our MPT() class.\n",
    "        artifacts={\"snapshot\": snapshot_location},\n",
    "        pip_requirements=[\n",
    "            f\"torch=={torch_version}\",\n",
    "            f\"transformers=={transformers.__version__}\",\n",
    "            f\"accelerate=={accelerate.__version__}\",\n",
    "            \"einops\",\n",
    "            \"sentencepiece\",\n",
    "        ],\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating an MPTForCausalLM model from /Users/benjamin.wilson/.cache/huggingface/modules/transformers_modules/mpt-7b/modeling_mpt.py\n",
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b97531220914ddbacf9628c5f252408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The execution of this is commented out for the purposes of runtime on CPU.\n",
    "# If you are running this on a system with a sufficiently powerful GPU, you may uncomment and interface with the model!\n",
    "\n",
    "# loaded_model.predict(pd.DataFrame(\n",
    "#     {\"prompt\": [\"What is machine learning?\"]}), params={\"temperature\": 0.6}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Through this tutorial, we've seen the power and flexibility of MLflow's custom `pyfunc`. By understanding the specific needs of our model and defining a custom `pyfunc` to cater to those needs, we can ensure a seamless deployment process and a user-friendly interface.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
